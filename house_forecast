import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('seaborn')
from scipy.stats import norm, skew
import numpy as np
import seaborn as sns

#读取数据
train=pd.read_csv("D:\\code\\python code\\Machine-Learning\\train.csv")
test=pd.read_csv("D:\\code\\python code\\Machine-Learning\\test.csv")
# print("Train set size:", train.shape)
# print("Test set size:", test.shape)
# print(train)
#测试数据散点图（生活面积-销售价格）
# plt.scatter(train.GrLivArea, train.SalePrice)
# # plt.show()

#清除明显不正常数据
train=train[train.GrLivArea<4500]
# plt.scatter(train.GrLivArea, train.SalePrice)
# plt.show()


#查看ID列是否有重复项
# print(len(np.unique(train['Id'])) == len(train))
# print(len(np.unique(test['Id'])) == len(test))

#无重复项，代表这列仅用来标识，并不是特征值可以删除
train = train.drop(['Id'], axis=1)
test = test.drop(['Id'], axis=1)
# print("Train set size:", train.shape)
# print("Test set size:", test.shape)

#合并数据，将SalePrice和LogSalePrice合并
df = pd.concat([train.SalePrice, np.log(train.SalePrice + 1).rename('LogSalePrice')], axis=1, names=['SalePrice', 'LogSalePrice'])
# print(df.head())

# plt.subplot(1, 2, 1)
# sns.distplot(train.SalePrice, kde=False, fit = norm)
#
# plt.subplot(1, 2, 2)
# sns.distplot(np.log(train.SalePrice + 1), kde=False, fit = norm)
# plt.xlabel('Log SalePrice')
# plt.show()
#上述图标显示未取对数数据明显右偏
train.SalePrice = np.log1p(train.SalePrice)


#Feature Engineering
#记录训练集的SalePrice=>>DataFrame
TrainSalePrice = train.SalePrice.reset_index(drop=True)
train_features = train.drop(['SalePrice'], axis=1)
test_features = test
# print(train_features.shape)
# print(test_features.shape)

#h合并训练集和测试集
features = pd.concat([train_features, test_features]).reset_index(drop=True)
# print(features.shape)

#处理空值
#筛选含空值列
nulls = np.sum(features.isnull())
nullcols = nulls.loc[(nulls != 0)]
#空值列的数据类型
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
#合并数据
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")

#填充数据
#用每列最多的数据填充
features['Functional'] = features['Functional'].fillna('Typ')
features['Electrical'] = features['Electrical'].fillna("SBrkr")
features['KitchenQual'] = features['KitchenQual'].fillna("TA")

features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])
features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])
#individually
# pd.set_option('max_columns', None)
# print(features[features['PoolArea'] > 0 & features['PoolQC'].isnull()])
#填充泳池空值
features.loc[2418, 'PoolQC'] = 'Fa'
features.loc[2501, 'PoolQC'] = 'Gd'
features.loc[2597, 'PoolQC'] = 'Fa'

#车库
features.loc[2124, 'GarageYrBlt'] = features['GarageYrBlt'].median()
features.loc[2574, 'GarageYrBlt'] = features['GarageYrBlt'].median()

features.loc[2124, 'GarageFinish'] = features['GarageFinish'].mode()[0]
features.loc[2574, 'GarageFinish'] = features['GarageFinish'].mode()[0]

features.loc[2574, 'GarageCars'] = features['GarageCars'].median()

features.loc[2124, 'GarageArea'] = features['GarageArea'].median()
features.loc[2574, 'GarageArea'] = features['GarageArea'].median()

features.loc[2124, 'GarageQual'] = features['GarageQual'].mode()[0]
features.loc[2574, 'GarageQual'] = features['GarageQual'].mode()[0]

features.loc[2124, 'GarageCond'] = features['GarageCond'].mode()[0]
features.loc[2574, 'GarageCond'] = features['GarageCond'].mode()[0]


basement_columns = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',
                   'BsmtFinType2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',
                   'TotalBsmtSF']

tempdf = features[basement_columns]
tempdfnulls = tempdf[tempdf.isnull().any(axis=1)]
#now select just the rows that have less then 5 NA's,
# meaning there is incongruency in the row.
# print(tempdfnulls[(tempdfnulls.isnull()).sum(axis=1) < 5])

#since smaller than SF1
features.loc[332, 'BsmtFinType2'] = 'ALQ'
features.loc[947, 'BsmtExposure'] = 'No'
features.loc[1485, 'BsmtExposure'] = 'No'
features.loc[2038, 'BsmtCond'] = 'TA'
features.loc[2183, 'BsmtCond'] = 'TA'

#v small basement so let's do Poor.
features.loc[2215, 'BsmtQual'] = 'Po'

#similar but a bit bigger.
features.loc[2216, 'BsmtQual'] = 'Fa'

#unfinished bsmt so prob not.
features.loc[2346, 'BsmtExposure'] = 'No'

#cause ALQ for bsmtfintype1
features.loc[2522, 'BsmtCond'] = 'Gd'


subclass_group = features.groupby('MSSubClass')
Zoning_modes = subclass_group['MSZoning'].apply(lambda x : x.mode()[0])
# print(Zoning_modes)

#根据建筑类型来填充销售类型
features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))
# pd.set_option('max_columns', None)
# print(features )

objects = []
for i in features.columns:
    if features[i].dtype == object:
        objects.append(i)

features.update(features[objects].fillna('None'))

nulls = np.sum(features.isnull())
nullcols = nulls.loc[(nulls != 0)]
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")

neighborhood_group = features.groupby('Neighborhood')
lot_medians = neighborhood_group['LotFrontage'].median()
# print(lot_medians)

features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

# pd.set_option('max_columns', None)
# print(features[(features['GarageYrBlt'].isnull()) & features['GarageArea'] > 0])

# pd.set_option('max_columns', None)
# features[(features['MasVnrArea'].isnull())]
# print(features[(features['MasVnrArea'].isnull())])
#上述可以推断没有不一致情况。可以安全的用0，代替
numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes:
        numerics.append(i)

features.update(features[numerics].fillna(0))

nulls = np.sum(features.isnull())
nullcols = nulls.loc[(nulls != 0)]
dtypes = features.dtypes
dtypes2 = dtypes.loc[(nulls != 0)]
info = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)
# print(info)
# print("There are", len(nullcols), "columns with missing values")
features.describe()
# print(features.describe())
features[features['GarageYrBlt'] == 2207]
# print(features[features['GarageYrBlt'] == 2207])
#2207可能是个错误，估算为2007
features.loc[2590, 'GarageYrBlt'] = 2007

# factors = ['MSSubClass', 'MoSold']
factors = ['MSSubClass']
for i in factors:
    features.update(features[i].astype('str'))
# print(features)


#对称度分析
numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
numerics2 = []
for i in features.columns:
    if features[i].dtype in numeric_dtypes:
        numerics2.append(i)

skew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)
skews = pd.DataFrame({'skew':skew_features})
# print(skews)

from scipy.special import boxcox1p
from scipy.stats import boxcox_normmax
high_skew = skew_features[skew_features > 0.5]
high_skew = high_skew
skew_index = high_skew.index

for i in skew_index:
    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))

skew_features2 = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)
skews2 = pd.DataFrame({'skew': skew_features2})
# print(skews2)

objects3 = []
for i in features.columns:
    if features[i].dtype == object:
        objects3.append(i)


# print("Training Set incomplete cases")

sums_features = features[objects3].apply(lambda x: len(np.unique(x)))
sums_features.sort_values(ascending=False)
# print(sums_features.sort_values(ascending=False))
# print(features['Street'].value_counts())
# print('-----')
# print(features['Utilities'].value_counts())
# print('-----')
# print(features['CentralAir'].value_counts())
# print('-----')
# print(features['PavedDrive'].value_counts())

#Utilities数据显示他是无效的
#features = features.drop(['Utilities'], axis=1)
features = features.drop(['Utilities', 'Street'], axis=1)
# print(features.shape)



#creat features
features['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +
                                 features['1stFlrSF'] + features['2ndFlrSF'])

features['Total_Bathrooms'] = (features['FullBath'] + (0.5*features['HalfBath']) +
                               features['BsmtFullBath'] + (0.5*features['BsmtHalfBath']))

features['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +
                              features['EnclosedPorch'] + features['ScreenPorch'] +
                             features['WoodDeckSF'])


#simplified features
features['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)
features['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)
features['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)
features['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)
# print(features.shape)
final_features = pd.get_dummies(features).reset_index(drop=True)
print(final_features.shape)
# print(TrainSalePrice.shape)
X = final_features.iloc[:len(TrainSalePrice),:]
testing_features = final_features.iloc[len(X):,:]

# print(X.shape)
# print(testing_features.shape)



import statsmodels.api as sm
# ols = sm.OLS(endog = TrainSalePrice, exog = X)
#
# fit = ols.fit()
# test2 = fit.outlier_test()['bonf(p)']
# outliers = list(test2[test2<1e-3].index)
# print(outliers)

outliers = [30, 88, 462, 631, 1322]
X = X.drop(X.index[outliers])
TrainSalePrice = TrainSalePrice.drop(TrainSalePrice.index[outliers])
overfit = []
for i in X.columns:
    counts = X[i].value_counts()
    zeros = counts.iloc[0]
    if zeros / len(X) * 100 >99.94:
        overfit.append(i)
overfit = list(overfit)
overfit.append('MSZoning_C (all)')
# print(overfit)


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.preprocessing import RobustScaler
from sklearn.pipeline import make_pipeline

#Build our model method
lm = LinearRegression()

#Build our cross validation method
kfolds = KFold(n_splits=10, shuffle=True, random_state=23)

#build our model scoring function
def cv_rmse(model):
    rmse = np.sqrt(-cross_val_score(model, X, TrainSalePrice,
                                   scoring="neg_mean_squared_error",
                                   cv = kfolds))
    return(rmse)


#second scoring metric
def cv_rmsle(model):
    rmsle = np.sqrt(np.log(-cross_val_score(model, X, TrainSalePrice,
                                           scoring = 'neg_mean_squared_error',
                                           cv=kfolds)))
    return(rmsle)

benchmark_model = make_pipeline(RobustScaler(),
                                lm).fit(X=X, y=TrainSalePrice)
cv_rmse(benchmark_model).mean()
# print(cv_rmse(benchmark_model).mean())
coeffs = pd.DataFrame(list(zip(X.columns, benchmark_model.steps[1][1].coef_)), columns=['Predictors', 'Coefficients'])

coeffs.sort_values(by='Coefficients', ascending=False)


#l2惩罚
from sklearn.linear_model import RidgeCV


def ridge_selector(k):
    ridge_model = make_pipeline(RobustScaler(),
                                RidgeCV(alphas=[k],
                                        cv=kfolds)).fit(X, TrainSalePrice)

    ridge_rmse = cv_rmse(ridge_model).mean()
    return (ridge_rmse)

r_alphas = [.0001, .0003, .0005, .0007, .0009,
          .01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 60, 70, 80]

ridge_scores = []
for alpha in r_alphas:
    score = ridge_selector(alpha)
    ridge_scores.append(score)
plt.plot(r_alphas, ridge_scores, label='Ridge')
plt.legend('center')
plt.xlabel('alpha')
plt.ylabel('score')
plt.show()
ridge_score_table = pd.DataFrame(ridge_scores, r_alphas, columns=['RMSE'])
# ridge_score_table
alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]

ridge_model2 = make_pipeline(RobustScaler(),
                            RidgeCV(alphas = alphas_alt,
                                    cv=kfolds)).fit(X, TrainSalePrice)
print("l2:")
print(cv_rmse(ridge_model2).mean())
print(ridge_model2.steps[1][1].alpha_)



#l1惩罚
from sklearn.linear_model import LassoCV

alphas = [0.00005, 0.0001, 0.0003, 0.0005, 0.0007,
          0.0009, 0.01]
alphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,
           0.0006, 0.0007, 0.0008]


lasso_model2 = make_pipeline(RobustScaler(),
                             LassoCV(max_iter=1e7,
                                    alphas = alphas2,
                                    random_state = 42)).fit(X, TrainSalePrice)
scores = lasso_model2.steps[1][1].mse_path_

plt.plot(alphas2, scores, label='Lasso')
plt.legend(loc='center')
plt.xlabel('alpha')
plt.ylabel('RMSE')
plt.tight_layout()
plt.show()

print("l1:")
print(lasso_model2.steps[1][1].alpha_)
print(cv_rmse(lasso_model2).mean())


coeffs = pd.DataFrame(list(zip(X.columns, lasso_model2.steps[1][1].coef_)), columns=['Predictors', 'Coefficients'])
used_coeffs = coeffs[coeffs['Coefficients'] != 0].sort_values(by='Coefficients', ascending=False)
# print(used_coeffs.shape)
# print(used_coeffs)
used_coeffs_values = X[used_coeffs['Predictors']]
# used_coeffs_values.shape

#Elastic Net (L1 and L2 penalty)
from sklearn.linear_model import ElasticNetCV

e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]
e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]

elastic_cv = make_pipeline(RobustScaler(),
                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,
                                        cv=kfolds, l1_ratio=e_l1ratio))

elastic_model3 = elastic_cv.fit(X, TrainSalePrice)
print("elastic_model3")
print(cv_rmse(elastic_model3).mean())
print(elastic_model3.steps[1][1].l1_ratio_)
print(elastic_model3.steps[1][1].alpha_)


#xgboost
from sklearn.model_selection import GridSearchCV
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 4
# %matplotlib inline
import xgboost as xgb
from xgboost import XGBRegressor

from sklearn.metrics import mean_squared_error


def modelfit(alg, dtrain, target, useTrainCV=True,
             cv_folds=5, early_stopping_rounds=50):
    if useTrainCV:
        xgb_param = alg.get_xgb_params()
        xgtrain = xgb.DMatrix(dtrain.values,
                              label=TrainSalePrice.values)

        print("\nGetting Cross-validation result..")
        cvresult = xgb.cv(xgb_param, xgtrain,
                          num_boost_round=alg.get_params()['n_estimators'],
                          nfold=cv_folds, metrics='rmse',
                          early_stopping_rounds=early_stopping_rounds,
                          verbose_eval=True)
        alg.set_params(n_estimators=cvresult.shape[0])

    # Fit the algorithm on the data
    print("\nFitting algorithm to data...")
    alg.fit(dtrain, target, eval_metric='rmse')

    # Predict training set:
    print("\nPredicting from training data...")
    dtrain_predictions = alg.predict(dtrain)

    # Print model report:
    print("\nModel Report")
    print("RMSE : %.4g" % np.sqrt(mean_squared_error(target.values,
                                                     dtrain_predictions)))

xgb3 = XGBRegressor(learning_rate =0.01, n_estimators=3460, max_depth=3,
                     min_child_weight=0 ,gamma=0, subsample=0.7,
                     colsample_bytree=0.7,objective= 'reg:linear',
                     nthread=4,scale_pos_weight=1,seed=27, reg_alpha=0.00006)

xgb_fit = xgb3.fit(X, TrainSalePrice)


#svm
from sklearn import svm
svr_opt = svm.SVR(C = 100000, gamma = 1e-08)

svr_fit = svr_opt.fit(X, TrainSalePrice)

#LGBM
from lightgbm import LGBMRegressor

lgbm_model = LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

print("lgbm_model")
print(cv_rmse(lgbm_model).mean())
lgbm_fit = lgbm_model.fit(X, TrainSalePrice)

#Stacking Generalization
from mlxtend.regressor import StackingCVRegressor
from sklearn.pipeline import make_pipeline

#setup models
ridge = make_pipeline(RobustScaler(),
                      RidgeCV(alphas = alphas_alt, cv=kfolds))

lasso = make_pipeline(RobustScaler(),
                      LassoCV(max_iter=1e7, alphas = alphas2,
                              random_state = 42, cv=kfolds))

elasticnet = make_pipeline(RobustScaler(),
                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,
                                        cv=kfolds, l1_ratio=e_l1ratio))

lightgbm = make_pipeline(RobustScaler(),
                        LGBMRegressor(objective='regression',num_leaves=5,
                                      learning_rate=0.05, n_estimators=720,
                                      max_bin = 55, bagging_fraction = 0.8,
                                      bagging_freq = 5, feature_fraction = 0.2319,
                                      feature_fraction_seed=9, bagging_seed=9,
                                      min_data_in_leaf =6,
                                      min_sum_hessian_in_leaf = 11))

xgboost = make_pipeline(RobustScaler(),
                        XGBRegressor(learning_rate =0.01, n_estimators=3460,
                                     max_depth=3,min_child_weight=0 ,
                                     gamma=0, subsample=0.7,
                                     colsample_bytree=0.7,
                                     objective= 'reg:linear',nthread=4,
                                     scale_pos_weight=1,seed=27,
                                     reg_alpha=0.00006))


#stack
stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,
                                            xgboost, lightgbm),
                               meta_regressor=xgboost,
                               use_features_in_secondary=True)

#prepare dataframes
stackX = np.array(X)
stacky = np.array(TrainSalePrice)

# scoring

print("cross validated scores")

for model, label in zip([ridge, lasso, elasticnet, xgboost, lightgbm, stack_gen],
                        ['RidgeCV', 'LassoCV', 'ElasticNetCV', 'xgboost', 'lightgbm',
                         'StackingCVRegressor']):
    SG_scores = cross_val_score(model, stackX, stacky, cv=kfolds,
                                scoring='neg_mean_squared_error')
    print("RMSE", np.sqrt(-SG_scores.mean()), "SD", scores.std(), label)

stack_gen_model = stack_gen.fit(stackX, stacky)

em_preds = elastic_model3.predict(testing_features)
lasso_preds = lasso_model2.predict(testing_features)
ridge_preds = ridge_model2.predict(testing_features)
stack_gen_preds = stack_gen_model.predict(testing_features)
xgb_preds = xgb_fit.predict(testing_features)
svr_preds = svr_fit.predict(testing_features)
lgbm_preds = lgbm_fit.predict(testing_features)

stack_preds = ((0.2*em_preds) + (0.1*lasso_preds) + (0.1*ridge_preds) +
               (0.2*xgb_preds) + (0.1*lgbm_preds) + (0.3*stack_gen_preds))
submission = pd.read_csv("../input/sample_submission.csv")
submission.iloc[:,1] = np.expm1(stack_preds)
submission.to_csv("final_submission.csv", index=False)
